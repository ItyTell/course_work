
\chapter{Чисельні методи}


У другому розділі розглянемо чисельні методи для розв'язання 
данних сисетем диф рівняннь та для визначення оптимальних параметрів, що 
якнайкраще симулюють розвиток захворювансті.

\section{Методи вирішення системи диф рівняннь}


Розв'язати подібні системи в загальному вигляді від параметрів не можна 
через нелінійність та не стійкість розв'язків, тож у данному розділі ми 
розглянемо деякі чисельні методи для розв'язку подібних систем відносно 
наперед відомих параметрів. 

\subsection{Метод Ейлера}


Метод Ейлера найбільш базовий чисельний метод для розв'язання нелінійних
систем диференціальних рівняннь, що застосувується при відомих початкових
значеннях. 
В данному випадку нам відома кількість хворих на початку епідемії.
Це частковий випадок методів Рунге Кутті. 
Загалом метод можна описати формулою $dS \approx S_{n+1} - S_n$.
Зазвичай береться певний крок $dt$ і дифиренціальне рівняння 
апроксимується відносно нього:

$$\frac{dS}{dt} = f(t, S) \Rightarrow 
S_{n+1} = S_n + f(t_n, S_n)$$

Ітерації починаються при відомому $S_0$. Метод є не стабільним при великому 
кроці $dt$.

Як приклад візьмемо SIR модель при розв'язанні моделі отримаємо:

$$
\begin{cases}
    S_{n+1} = S_n - \frac{\beta}{N} S_n I_n dt \\
    I_{n+1} = I_n + \frac{\beta}{N} S_n I_n dt - \gamma I_n dt \\
    R_{n+1} = R_n + \gamma I_n dt \\
    S_0 = N - 1, I_0 = 1, R_0 = 0
\end{cases}
$$


Функція написана на С для отримання данних з моделі відносно її параметрів 
кроку та початкових умов. 
\lstinputlisting[style=CStyle]{main.c}

Мова програмування С була обрана для написання 
цієї функції завдяки її швидкості виконання та відносної простоти 
написанння коду. Потреба у високій швидкості виконання обумовлена потребами 
задля швидкого знахордження оптимальних параметрів - через складну 
залежність від параметрів моделі використати градієнтні методи не вийде, а
не градієнтні методи можуть багато разів викликати данну функцію на кожній 
ітерації. 


Функції, що розв'язують інші моделі відрізняються лише кількістю масивів та 
основним циклом, загальна структура функцій однакова. 
Час виконання програми такий то. 


\subsection{Класичний метод Рунге Кутті}

Ідеологічно методи Рунге Кутті це алгоритми влучного застосування методу 
Ейлера певну кількість разів. Від кількості разів, яку він застосовується 
залежить порядок методу. Наприклад класичним методом Рунге Кутті називають 
саме метод четвертого порядку. 


Нехай у нас є система диференціальних рівняннь $\frac{dx}{dt} = f(x, t)$.
Тоді, якщо скористатися методом Ейлера отримаємо 
$x_{k+1} = x_k + f(x_k, t_k)$. 
Замість цього ми рахуємо лише доданок $k_1 = f(x_k, t_k)$. 
Далі рахуємо, ще декілька доданків 


$$ \begin{cases}
    k_1 = f(x_k, t_k) \\
    k_2 = f(x_k + k_1\frac{dt}{2}, t_k + \frac{dt}{2}) \\
    k_3 = f(x_k + k_2\frac{dt}{2}, t_k + \frac{dt}{2})\\
    k_4 = f(x_k + k_3 dt, t_k + dt)
\end{cases}$$


І вже тоді додаємо, отримуючи наближене значення


$$x_{k+1} = x_k + \frac{dt}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$


Розглянемо роботу методу на звичайній SIR моделі. Для простоти введемо 
функції:

$$
\begin{cases}
    f_1(S_k, I_k, R_k) = - \beta \frac{S_k}{N} I_k \\
    f_2(S_k, I_k, R_k) =   \beta \frac{S_k}{N} I_k - \gamma I_k \\
    f_3(S_k, I_k, R_k) =   \gamma I_k 
\end{cases}$$

Розрахунок наближень 4-рьох порядків:
$$
\begin{cases}
    s_1 = f_1(S_k, I_k, R_k) \\
    i_1 = f_2(S_k, I_k, R_k) \\
    r_1 = f_3(S_k, I_k, R_k)
\end{cases}
$$
$$
\begin{cases}
    s_2 = f_1(S_k + \frac{dt}{2} s_1, I_k + \frac{dt}{2} i_1, R_k + 
    \frac{dt}{2} r_1) \\
    i_2 = f_2(S_k + \frac{dt}{2} s_1, I_k + \frac{dt}{2} i_1, R_k + 
    \frac{dt}{2} r_1) \\
    r_2 = f_3(S_k + \frac{dt}{2} s_1, I_k + \frac{dt}{2} i_1, R_k + 
    \frac{dt}{2} r_1) 
\end{cases}$$ $$
\begin{cases}
    s_3 = f_1(S_k + \frac{dt}{2} s_2, I_k + \frac{dt}{2} i_2, R_k + 
    \frac{dt}{2} r_2) \\
    i_3 = f_2(S_k + \frac{dt}{2} s_2, I_k + \frac{dt}{2} i_2, R_k + 
    \frac{dt}{2} r_2) \\
    r_3 = f_3(S_k + \frac{dt}{2} s_2, I_k + \frac{dt}{2} i_2, R_k + 
    \frac{dt}{2} r_2) 
\end{cases}
$$


$$
\begin{cases}
    s_4 = f_1(S_k + dt s_3, I_k + dt i_3, R_k + r_3) \\
    i_4 = f_2(S_k + dt s_3, I_k + dt i_3, R_k + r_3) \\
    r_4 = f_3(S_k + dt s_3, I_k + dt i_3, R_k + r_3) 
\end{cases}$$ 
Отримане наближення:
$$
\begin{cases}
    S_{k + 1} = S_k + \frac{dt}{6} (s_1 + 2s_2 + 2s_3 + s_4)\\
    I_{k + 1} = I_k + \frac{dt}{6} (i_1 + 2i_2 + 2i_3 + i_4)\\
    R_{k + 1} = R_k + \frac{dt}{6} (r_1 + 2r_2 + 2r_3 + r_4)
\end{cases}
$$ 


Задля застосуванння данного методу на інших моделях треба лише 
додати потрібні допоміжні функції та розрахунок додаткових змінних, принцип 
залишається незмінним.


\section{Методи підбору параметрів}


Тепер коли ми можемо порахувати різницю між прогнозованою величиною, 
яку видає наша модель і реальними данними, можемо використати алгоритми 
оптимізації для підбору оптимальних параметрів. За функцію ціни візьмемо 
відстань між графіками реальних данних та прогнозованих. 
Розглянемо лише неградієнтні методи оптимізації, адеж порахувати градієнт 
функції наближеної методами з першої частини глави не можна через 
складність обрахунків та кількість задіяної пам'яті.


\subsection{Метод рою часток}


Метод рою частинок не передбачає розрахунок градієнта функції, тому його 
швидкість роботи більш ніж задовільняє нашу задачу.
Данний метод використовує "частинки", кожна з яких символізує один
розв'язок задачі. На кожній ітерації частинка намагається рухатись у 
оптимальному напрямі, тут присутній елемент стахастики адже випадковим є те 
чи буде вона рухатись до глобального оптимума (за весь час 
роботи алгоритма) чи до локального оптимума данної частинки. 
Функцію зміни параметрів частинки можна описати такою формулою: 

$$\Delta x_{k + 1}^{(i)} = \alpha \Delta x_k^{(i)} + 
\beta (x_{best}^{(i)} - x_k) + \gamma(x_{best} - x_k^{(i)})$$


У формулі $x_{best}^{(i)}$ найкраще значення цієї частинки, 
$x_{best}$ найкраще значення всіх частинок. 


Моя реалізація на мові Python:

\begin{lstlisting}[language=Python, caption=Python example]
    def roy(f, n, N, iteration, w, 
    a1, a2, A, B, eps, first):
        X = np.array([[random.uniform(A[i], B[i]) 
        for i in range(n)] for j in range(N)])
        V = np.array([[random.uniform(-eps, eps) 
        for i in range(n)] 
        for j in range(N)])
        for i in range(n):
            X[0][i] = first[i]
        P = np.copy(X)
        res = np.array([f(X[i]) for i in range(N)])
        b = P[np.argmin(res)]
        for i in tqdm(range(iteration)):
            V = w * V + 
            a1 * random.random() * (P - X) + 
            a2 * random.random() * (b - X)
            X = X + V
            for j in range(N):
                res1 = f(X[j])
                if res1 < res[j]:
                    P[j] = X[j]
                    res[j] = res1
            b = P[np.argmin(res)]
        return b
\end{lstlisting}


